{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self attention\n",
    "sequence $x$ is $b x t x k$. batch sequence length and dimension\n",
    "Follwoing we add linear transformations in order to provide learnable parameters.\n",
    "* $W_q$ transform $x_i$ to $q_i$ query vector. Compared to every other vector inorder to establish weights for its own output $y_i$\n",
    "* $W_k$ transform $x_i$ to $k_i$ key vector. Used for $x_i$ being compared to the queries, to establish the weights for other $y_j$\n",
    "* $W_v$ transform $x_i$ to $v_i$ vector to be weighted and that actually encodes the information.\n",
    "\n",
    "<img src=\"images/learnable-weights.png\"  width=\"500\" height=\"600\">\n",
    "\n",
    "Hence, the self attention layer for each $y_i$ will be the following\n",
    "\n",
    "\n",
    "<img src=\"images/learnable-structure.png\"  width=\"500\" height=\"600\">\n",
    "\n",
    "Small tricks:\n",
    "\n",
    "* Divide by $\\sqrt{k}$ to reduce input values of the softmax (as dimension increases it reduces by the euclidean lenght, read bibliography for more explanations)\n",
    "* Multi-head attention: learn many query, keys values for each input (paralllel self attention) and concatenate at the end. Allows to focus and learn different queries for each input. Each $W_q^r$, $W_k^r$ and $W_v^r$ is an attention head.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, k, heads=10):\n",
    "        super().__init__()\n",
    "        #Compute Linear Transformations\n",
    "        self.transform_queries = nn.Linear(k, k*heads, bias=False)\n",
    "        self.transform_keys = nn.Linear(k, k*heads, bias=False)\n",
    "        self.transform_values = nn.Linear(k, k*heads, bias=False)\n",
    "        # Linear Transform that reduces dimensionality\n",
    "        self.dimension_reduce = nn.Linear(k*heads, k, bias=False)\n",
    "        self.k = k\n",
    "        self.h = heads\n",
    "        \n",
    "    def forward(self,x): #input b x t x k\n",
    "        b, t, k = x.size()\n",
    "        h = self.h\n",
    "        \n",
    "        #Transform: b,t,k => b,t,k*h \n",
    "        queries = self.transform_queries(x)\n",
    "        keys = self.transform_keys(x)\n",
    "        values = self.transform_values(x)\n",
    "        #Separate heads from dimension\n",
    "        # b,t,k*h => b,t,h,k\n",
    "        queries = queries.view(b,t,h,k)\n",
    "        keys = keys.view(b,t,h,k)\n",
    "        values = values.view(b,t,h,k)\n",
    "        #Matrix multiplication for each batch and each head. Hence we merge heads and batch in order tu use torch.bmm\n",
    "        # Transpose b,t,h,k => b,h,t,k\n",
    "        # Merge dim b,h,t,k => b*h,t,k\n",
    "        queries = queries.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        keys = keys.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        values = values.transpose(1,2).contiguous().view(b*h,t,k)\n",
    "        # Scale\n",
    "        queries = queries /(k**(0.25))\n",
    "        keys = keys/(k**(0.25))\n",
    "        \n",
    "        #Use torch batch matrix mult, performs a matrix mult for each elemt of the batch.\n",
    "        #Batch consist of batch sample and head\n",
    "        # b*h,t,k x b*h,k,t => b*h,t,t \n",
    "        weights = torch.bmm(queries,keys.transpose(1,2))\n",
    "        soft_weights = F.softmax(weights, dim=2)\n",
    "        #Multiply weights b*h,t,t x b*h,t,k => b*h,t,k \n",
    "        #(each row weight contains weights for each row vectors, row weights linear combination of rowvectors) \n",
    "        output = torch.bmm(soft_weights, values).view(b, h, t, k)\n",
    "        #Merge the h heads on the k dimension\n",
    "        #Transpose b,h,t,k =>  b,t,h,k \n",
    "        #View(merge) b,t,h,k => b,t,h*k \n",
    "        output = output.transpose(1,2).contiguous().view(b,t,h*k)\n",
    "        #Reduce dimension b,t,h*k => b,t,k\n",
    "        return self.dimension_reduce(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0296,  0.0317,  0.1882,  0.0721, -0.2167, -0.1055,  0.0324,\n",
       "           -0.0840,  0.1289, -0.0255],\n",
       "          [ 0.0317,  0.0318,  0.1885,  0.0736, -0.2177, -0.1046,  0.0331,\n",
       "           -0.0845,  0.1283, -0.0263],\n",
       "          [ 0.0310,  0.0303,  0.1882,  0.0735, -0.2160, -0.1048,  0.0335,\n",
       "           -0.0832,  0.1291, -0.0247],\n",
       "          [ 0.0298,  0.0334,  0.1883,  0.0719, -0.2170, -0.1078,  0.0327,\n",
       "           -0.0856,  0.1299, -0.0260],\n",
       "          [ 0.0306,  0.0320,  0.1886,  0.0722, -0.2164, -0.1058,  0.0328,\n",
       "           -0.0838,  0.1290, -0.0247]],\n",
       " \n",
       "         [[ 0.0150,  0.0536,  0.2762,  0.0174, -0.2163, -0.1906, -0.0490,\n",
       "           -0.1417,  0.1454, -0.0242],\n",
       "          [ 0.0178,  0.0540,  0.2771,  0.0179, -0.2177, -0.1874, -0.0473,\n",
       "           -0.1413,  0.1419, -0.0235],\n",
       "          [ 0.0167,  0.0547,  0.2763,  0.0187, -0.2182, -0.1894, -0.0504,\n",
       "           -0.1425,  0.1438, -0.0251],\n",
       "          [ 0.0171,  0.0545,  0.2762,  0.0186, -0.2178, -0.1880, -0.0506,\n",
       "           -0.1417,  0.1442, -0.0253],\n",
       "          [ 0.0174,  0.0522,  0.2750,  0.0187, -0.2155, -0.1882, -0.0467,\n",
       "           -0.1408,  0.1441, -0.0240]],\n",
       " \n",
       "         [[ 0.1057, -0.0146,  0.2985,  0.0393, -0.2648, -0.0818, -0.0125,\n",
       "           -0.1568,  0.0849,  0.0039],\n",
       "          [ 0.1060, -0.0135,  0.2979,  0.0393, -0.2643, -0.0823, -0.0135,\n",
       "           -0.1570,  0.0852,  0.0044],\n",
       "          [ 0.1068, -0.0164,  0.2995,  0.0391, -0.2661, -0.0813, -0.0111,\n",
       "           -0.1576,  0.0844,  0.0031],\n",
       "          [ 0.1075, -0.0164,  0.2985,  0.0402, -0.2654, -0.0817, -0.0126,\n",
       "           -0.1574,  0.0847,  0.0020],\n",
       "          [ 0.1068, -0.0147,  0.2978,  0.0401, -0.2630, -0.0834, -0.0130,\n",
       "           -0.1563,  0.0845,  0.0034]],\n",
       " \n",
       "         [[-0.0106, -0.0697,  0.2444, -0.0240, -0.2368, -0.1500, -0.0813,\n",
       "           -0.0614,  0.2026, -0.0179],\n",
       "          [-0.0064, -0.0715,  0.2453, -0.0242, -0.2377, -0.1527, -0.0778,\n",
       "           -0.0608,  0.2039, -0.0172],\n",
       "          [-0.0085, -0.0715,  0.2439, -0.0240, -0.2381, -0.1566, -0.0783,\n",
       "           -0.0622,  0.2042, -0.0160],\n",
       "          [-0.0082, -0.0705,  0.2458, -0.0243, -0.2364, -0.1524, -0.0771,\n",
       "           -0.0617,  0.2037, -0.0173],\n",
       "          [-0.0077, -0.0738,  0.2440, -0.0254, -0.2394, -0.1557, -0.0780,\n",
       "           -0.0607,  0.2034, -0.0167]],\n",
       " \n",
       "         [[ 0.1140,  0.0968,  0.3319,  0.0406, -0.2556, -0.1054,  0.0314,\n",
       "           -0.1359,  0.1326, -0.0462],\n",
       "          [ 0.1129,  0.0961,  0.3308,  0.0401, -0.2565, -0.1064,  0.0319,\n",
       "           -0.1368,  0.1327, -0.0470],\n",
       "          [ 0.1124,  0.0949,  0.3304,  0.0390, -0.2548, -0.1101,  0.0338,\n",
       "           -0.1373,  0.1333, -0.0475],\n",
       "          [ 0.1120,  0.0968,  0.3311,  0.0398, -0.2548, -0.1082,  0.0309,\n",
       "           -0.1370,  0.1334, -0.0471],\n",
       "          [ 0.1122,  0.0956,  0.3309,  0.0395, -0.2537, -0.1087,  0.0334,\n",
       "           -0.1374,  0.1324, -0.0470]],\n",
       " \n",
       "         [[ 0.0146,  0.0244,  0.2108, -0.0143, -0.1956, -0.1198, -0.0204,\n",
       "           -0.0487,  0.1526, -0.0493],\n",
       "          [ 0.0165,  0.0256,  0.2114, -0.0172, -0.1991, -0.1212, -0.0205,\n",
       "           -0.0491,  0.1487, -0.0479],\n",
       "          [ 0.0133,  0.0264,  0.2094, -0.0140, -0.1970, -0.1235, -0.0193,\n",
       "           -0.0473,  0.1515, -0.0480],\n",
       "          [ 0.0159,  0.0266,  0.2092, -0.0164, -0.1961, -0.1225, -0.0228,\n",
       "           -0.0467,  0.1522, -0.0488],\n",
       "          [ 0.0150,  0.0242,  0.2115, -0.0155, -0.1963, -0.1195, -0.0199,\n",
       "           -0.0494,  0.1500, -0.0479]],\n",
       " \n",
       "         [[ 0.0812,  0.0771,  0.2757,  0.0263, -0.2984, -0.1357, -0.0173,\n",
       "           -0.1941,  0.1291, -0.0705],\n",
       "          [ 0.0827,  0.0757,  0.2763,  0.0277, -0.3004, -0.1338, -0.0170,\n",
       "           -0.1921,  0.1298, -0.0718],\n",
       "          [ 0.0803,  0.0773,  0.2758,  0.0264, -0.3001, -0.1367, -0.0186,\n",
       "           -0.1940,  0.1292, -0.0702],\n",
       "          [ 0.0841,  0.0774,  0.2742,  0.0254, -0.2972, -0.1367, -0.0187,\n",
       "           -0.1936,  0.1285, -0.0730],\n",
       "          [ 0.0822,  0.0773,  0.2755,  0.0258, -0.2970, -0.1373, -0.0172,\n",
       "           -0.1944,  0.1282, -0.0716]],\n",
       " \n",
       "         [[ 0.0693,  0.0812,  0.2697,  0.0324, -0.1886, -0.1145,  0.0055,\n",
       "           -0.1472,  0.1425, -0.0547],\n",
       "          [ 0.0680,  0.0804,  0.2694,  0.0310, -0.1884, -0.1141,  0.0071,\n",
       "           -0.1463,  0.1418, -0.0537],\n",
       "          [ 0.0705,  0.0801,  0.2702,  0.0314, -0.1889, -0.1141,  0.0074,\n",
       "           -0.1469,  0.1406, -0.0521],\n",
       "          [ 0.0700,  0.0807,  0.2709,  0.0325, -0.1884, -0.1152,  0.0071,\n",
       "           -0.1471,  0.1411, -0.0547],\n",
       "          [ 0.0689,  0.0816,  0.2703,  0.0330, -0.1887, -0.1162,  0.0060,\n",
       "           -0.1482,  0.1419, -0.0534]]], grad_fn=<UnsafeViewBackward>),\n",
       " torch.Size([8, 5, 10]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fast check\n",
    "self_attention = SelfAttention(k=10,heads=20)\n",
    "sample_input = torch.rand(8,5,10)\n",
    "output = self_attention(sample_input)\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "\"Any architecture designed to process a connected set of units—such where the only interaction between units is through self-attention\"\n",
    "<img src=\"images/transformers.png\"  width=\"500\" height=\"600\">\n",
    "\n",
    "It is to say seq-to-seq all at once\n",
    "\n",
    "Basic structure:\n",
    "* Self attention\n",
    "* Norm layer\n",
    "* FeedForward Net\n",
    "* Norm Layer\n",
    "\n",
    "Additionaly residual blocks and keypoint is to combine feedforward with self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, k, heads, hidden_layer_mult=4):\n",
    "        super().__init__()\n",
    "        self.self_attention = SelfAttention(k, heads)\n",
    "        self.layer_norm_1 = nn.LayerNorm(k)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(k, hidden_layer_mult * k),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_mult * k, k),\n",
    "        )\n",
    "        self.layer_norm_2 = nn.LayerNorm(k)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention = self.self_attention(x)\n",
    "        norm = self.layer_norm_1(attention + x)\n",
    "        ff_output = self.feed_forward(norm)\n",
    "        output = self.layer_norm_2(ff_output+norm)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.8233, -1.3898, -1.3552,  0.6779,  0.1417, -1.2642,  1.5271,\n",
       "            0.8112, -0.4786,  0.5067],\n",
       "          [ 0.6343, -0.0228, -2.2422,  1.1804, -0.8153,  0.3628,  1.2845,\n",
       "           -0.4620,  0.5467, -0.4664],\n",
       "          [ 0.6151, -2.0556, -0.0430,  0.2308, -0.3274, -1.1467, -0.3801,\n",
       "            1.4909,  0.4577,  1.1584],\n",
       "          [-0.7969, -0.4526, -0.9660, -1.2747, -0.7350,  1.1137,  0.9998,\n",
       "           -0.5436,  1.3457,  1.3098],\n",
       "          [-1.6160,  1.1272, -0.5850,  0.0466, -1.1241,  0.3151,  1.6314,\n",
       "           -0.9376,  0.9011,  0.2412]],\n",
       " \n",
       "         [[ 1.3050, -2.1407, -0.7351,  0.0604,  1.0004,  0.2593,  0.2542,\n",
       "           -0.2618,  1.1129, -0.8547],\n",
       "          [-0.2232, -0.6451, -2.3826, -0.3343,  0.0539,  0.5688,  1.5108,\n",
       "            0.7899,  0.7139, -0.0521],\n",
       "          [ 0.5460, -1.1643, -1.9265, -0.5806,  0.4076,  0.2140,  0.9163,\n",
       "           -0.7309,  1.0479,  1.2703],\n",
       "          [ 0.7612, -0.3981, -1.6886, -0.4407,  0.5392, -1.0161,  1.1695,\n",
       "            1.8046, -0.3393, -0.3917],\n",
       "          [-0.1325, -0.4683, -1.7936,  1.2739,  0.3564, -1.3084,  0.9620,\n",
       "           -0.7092,  0.9012,  0.9185]],\n",
       " \n",
       "         [[-0.1581,  0.0658, -2.2194,  0.5996,  0.3835,  0.9014,  1.3304,\n",
       "           -1.0263,  0.7305, -0.6073],\n",
       "          [-0.0785, -0.8782, -2.1092, -0.2047,  0.1603,  1.1422,  1.1432,\n",
       "           -0.2062, -0.3564,  1.3876],\n",
       "          [ 0.7365, -2.1201, -0.1301,  0.3510,  0.2848, -1.4965,  1.2332,\n",
       "           -0.1223,  0.9216,  0.3420],\n",
       "          [-0.1878,  0.0336, -2.2689, -1.3764,  0.3229,  0.3886,  1.1771,\n",
       "            0.6328,  0.8168,  0.4612],\n",
       "          [-0.4262, -1.3658, -2.0315,  0.4204, -0.4638,  0.3817,  1.1552,\n",
       "            0.4717,  0.8802,  0.9780]],\n",
       " \n",
       "         [[-1.1822, -0.8164, -0.1014,  1.1324, -0.2604, -1.8415,  1.0200,\n",
       "           -0.0041,  1.1595,  0.8942],\n",
       "          [ 0.9442, -1.3084, -1.8303,  0.2128,  0.3368, -0.6616,  1.6027,\n",
       "           -0.4562,  0.5774,  0.5827],\n",
       "          [ 0.4169, -1.9759, -0.7382, -0.0089,  0.9715, -0.1724,  1.8236,\n",
       "            0.7073, -0.6756, -0.3483],\n",
       "          [ 0.4672, -1.2589, -2.2758,  0.4713,  1.2096,  0.5185,  0.1373,\n",
       "            0.8030, -0.4817,  0.4095],\n",
       "          [-1.4333, -0.4537, -1.3961, -1.2079,  0.8541,  1.1665,  0.0483,\n",
       "            1.1925,  0.7897,  0.4397]],\n",
       " \n",
       "         [[ 0.3048, -0.6371, -1.3183,  0.0955,  0.8761, -1.6722,  0.2695,\n",
       "            1.6448, -0.5930,  1.0300],\n",
       "          [-0.3312, -0.6892, -2.3423, -0.0291,  1.0304, -0.4681,  0.5472,\n",
       "            1.3344,  0.2317,  0.7163],\n",
       "          [ 0.4913, -0.2673, -0.5187,  0.7462, -0.7035, -2.0065, -0.4686,\n",
       "           -0.1283,  1.5437,  1.3118],\n",
       "          [-1.3433, -1.1404, -0.9957,  1.1774,  1.6687,  0.3149,  0.1006,\n",
       "            0.4307,  0.7349, -0.9477],\n",
       "          [ 0.2567, -0.6705, -1.6366, -1.2840,  0.4803,  1.6411,  1.4263,\n",
       "            0.1388,  0.0668, -0.4189]],\n",
       " \n",
       "         [[ 1.0457, -1.1261, -0.7114, -0.5580, -0.0197, -1.6807,  1.0873,\n",
       "            0.1546,  0.1440,  1.6641],\n",
       "          [-0.0813, -1.3503,  0.1493,  0.1651, -0.0703, -1.5206, -0.2018,\n",
       "           -0.2648,  1.0157,  2.1589],\n",
       "          [ 0.7573, -1.0333, -1.8489, -0.9040,  0.3669,  0.6463,  0.7330,\n",
       "           -0.6822,  0.4178,  1.5471],\n",
       "          [ 0.4748, -0.8216, -2.0361, -0.0105,  0.5515,  0.2776,  1.9049,\n",
       "           -0.4318, -0.5680,  0.6592],\n",
       "          [-1.3435,  0.4269, -2.0699, -0.6696,  0.6756,  0.1162,  0.6105,\n",
       "            0.0953,  1.3009,  0.8576]],\n",
       " \n",
       "         [[ 0.0810, -1.4011, -1.1792, -0.8514,  0.4430, -0.1203,  1.7279,\n",
       "            1.5864, -0.4270,  0.1406],\n",
       "          [ 0.2508, -0.9020, -2.0600,  1.1344,  1.0863, -0.5555,  0.9604,\n",
       "           -0.1831, -0.6113,  0.8800],\n",
       "          [-0.8423,  0.1984, -1.0199, -0.6789,  0.2731, -1.5613, -0.0554,\n",
       "            1.4414,  0.5578,  1.6870],\n",
       "          [ 0.3758, -0.7329, -1.6657,  0.6487,  1.1059, -1.4528,  1.0934,\n",
       "           -0.4500,  1.1767, -0.0991],\n",
       "          [ 0.2732, -1.9624, -1.1987,  0.9577, -0.2566, -0.6598,  1.1344,\n",
       "            1.2522,  0.5894, -0.1295]],\n",
       " \n",
       "         [[ 0.4646, -1.4339, -1.4919, -0.9969, -0.1629,  0.2920,  1.2247,\n",
       "            0.0082,  0.4565,  1.6396],\n",
       "          [ 0.1842, -1.3501, -1.6574,  0.5142, -0.5015,  0.2949,  1.8474,\n",
       "            0.2411, -0.5710,  0.9981],\n",
       "          [ 1.5624, -0.4599, -1.6278, -1.2388,  0.4168,  0.1282,  0.1269,\n",
       "           -0.0200, -0.5251,  1.6372],\n",
       "          [ 0.3614, -0.6248, -2.4826,  0.3388,  0.3294, -0.8051,  1.2211,\n",
       "            0.7019,  0.4802,  0.4796],\n",
       "          [-0.3419, -1.1058, -0.8274, -0.3372,  0.6737, -0.8404,  1.3371,\n",
       "            2.0946, -0.7223,  0.0697]]], grad_fn=<AddcmulBackward>),\n",
       " torch.Size([8, 5, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fast check\n",
    "transformer_module = TransformerModule(k=10, heads=20)\n",
    "sample_input = torch.rand(8,5,10)\n",
    "output = transformer_module(sample_input)\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
