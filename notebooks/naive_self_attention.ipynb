{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation that simulates the following procedure \n",
    "\n",
    "\n",
    "<img src=\"images/self-attention.svg\"  width=\"500\" height=\"600\">\n",
    "\n",
    "\n",
    "As you see in the picture it does not have any linear transformation to the inputs, hence no learneable parameters. In order to compute $y_2$, the self attention module uses $x_2$ and compares to all other inputs through a dot product. Then each input vector is weighted and sym by the dot product.\n",
    "Complex implementation uses a linear transformation for obtaining key vectors for each input, query vectors for each input as well (that is used for comparing the key vectors of other inputs) and value vectors that take sthe function of an embeding of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.7347, 0.6824, 0.6742, 1.3225],\n",
       "          [0.6824, 0.3378, 0.3318, 0.6420],\n",
       "          [0.6742, 0.3318, 1.1839, 1.1926],\n",
       "          [1.3225, 0.6420, 1.1926, 1.7248]],\n",
       " \n",
       "         [[1.8268, 1.5288, 1.6497, 1.3651],\n",
       "          [1.5288, 2.3350, 1.8272, 2.3138],\n",
       "          [1.6497, 1.8272, 1.9624, 1.9365],\n",
       "          [1.3651, 2.3138, 1.9365, 2.4830]]]),\n",
       " torch.Size([2, 4, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((2,4,5)) #size b,t,k => b=batch=2, t=sequence length=4, k=inmput space=5\n",
    "#torch batch mult, applies matrix mult over batch\n",
    "#dot products for all inputs, row of dot products weights\n",
    "w = torch.bmm(x,x.transpose(1,2))\n",
    "w, w.shape # (2,4,4) each position is a weight = a dot product of an reference input with the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4242, 0.1481, 0.1469, 0.2809],\n",
       "          [0.2965, 0.2100, 0.2088, 0.2847],\n",
       "          [0.1979, 0.1405, 0.3294, 0.3323],\n",
       "          [0.2577, 0.1305, 0.2263, 0.3854]],\n",
       " \n",
       "         [[0.3115, 0.2312, 0.2610, 0.1963],\n",
       "          [0.1475, 0.3303, 0.1988, 0.3234],\n",
       "          [0.2043, 0.2440, 0.2794, 0.2722],\n",
       "          [0.1189, 0.3070, 0.2105, 0.3636]]]),\n",
       " torch.Size([2, 4, 4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_weights = F.softmax(w, dim=2)\n",
    "softmax_weights, softmax_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfatten = torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
