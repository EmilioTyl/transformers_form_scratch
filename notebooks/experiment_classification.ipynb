{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment IMDB anlysis with Transformers form scratch\n",
    "Each sample is a sequence of embedded words. Output two positive/negative class\n",
    "Input:\n",
    "* batch of different sample sentences\n",
    "* sample is a sentence of dim t x embedding_size (where t is max sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from _context import src\n",
    "from src.models.model_utils import device_selection\n",
    "from src.models.predict_model import ClassSequenceTransformer\n",
    "#from src.models import TransformerClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "import random, tqdm, sys, math, gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "vocab_size = 50000\n",
    "batch_size = 8\n",
    "num_classes = 2\n",
    "embeding_size = 128\n",
    "transformer_heads = 8\n",
    "depth = 4\n",
    "lr = 1e-4\n",
    "warm = 10000\n",
    "epochs =100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "label = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets.IMDB.splits(text, label)\n",
    "text.build_vocab(train, max_size=vocab_size - 2)\n",
    "label.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=batch_size, device=device_selection())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine maximum sequence\n",
    "max_sequence = max([seq.text[0].size(1) for seq in train_iter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [torchtext.data.batch.Batch of size 8 from IMDB]\n",
       " \t[.text]:('[torch.LongTensor of size 8x402]', '[torch.LongTensor of size 8]')\n",
       " \t[.label]:[torch.LongTensor of size 8],\n",
       " 402)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore structure\n",
    "for seq in train_iter:\n",
    "    s = seq\n",
    "    break\n",
    "seq, seq.text[0].size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 402])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.text[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Transformer\n",
    "\n",
    "* Seq-to-seq model that does an global average pooling, the resulting vector is projected to a smaller dimension and  at the end and apply a softmax\n",
    "\n",
    "* Positional embeding: each transformer is positional invariant (different order of the inputs produce the same output). Add positional information to the input vectrors by generating positional vectors that are added to the input words. Positional embeding let the model to learn the vector value for position ( does not work well in  sequence lenghts the network has nott seen before). Positional encoding just add a funtion that map position to a vectors, but the network does not learn the mapping, it is an complex hyperparam to fix.\n",
    "\n",
    "<img src=\"images/transformer_model.svg\"  width=\"500\" height=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassSequenceTransformer(\n",
    "                                    num_classes=num_classes,\n",
    "                                    embeding_size=embeding_size,\n",
    "                                    transformer_heads=transformer_heads,\n",
    "                                    depth=depth,\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    max_sequence=max_sequence,\n",
    "                                    \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i:min(i/(warm/batch_size), 1.0)) # lr times lambda(iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.2967316210269928\n",
      "Acc val: 1.0\n",
      "Val loss  0.2892437279224396\n",
      "Epoch: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.07365045696496964\n",
      "Acc val: 1.0\n",
      "Val loss  0.07171900570392609\n",
      "Epoch: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:24<00:00,  2.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.02310607209801674\n",
      "Acc val: 1.0\n",
      "Val loss  0.022653289139270782\n",
      "Epoch: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.81it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.009804179891943932\n",
      "Acc val: 1.0\n",
      "Val loss  0.009660748764872551\n",
      "Epoch: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.62it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.005123793613165617\n",
      "Acc val: 1.0\n",
      "Val loss  0.00506583834066987\n",
      "Epoch: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.38it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.003079689107835293\n",
      "Acc val: 1.0\n",
      "Val loss  0.0030519049614667892\n",
      "Epoch: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.72it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.0020407685078680515\n",
      "Acc val: 1.0\n",
      "Val loss  0.0020256286952644587\n",
      "Epoch: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.74it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.0014514847425743937\n",
      "Acc val: 1.0\n",
      "Val loss  0.0014424878172576427\n",
      "Epoch: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.0010884369257837534\n",
      "Acc val: 1.0\n",
      "Val loss  0.0010826787911355495\n",
      "Epoch: 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:28<00:00,  1.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss  0.000849991396535188\n",
      "Acc val: 1.0\n",
      "Val loss  0.0008460917742922902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Overfit batch\n",
    "model = ClassSequenceTransformer(\n",
    "                                    num_classes=num_classes,\n",
    "                                    embeding_size=embeding_size,\n",
    "                                    transformer_heads=transformer_heads,\n",
    "                                    depth=depth,\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    max_sequence=max_sequence,\n",
    "                                    \n",
    "                                )\n",
    "optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i:min(i/(warm/batch_size), 1.0)) # lr times lambda(iter)\n",
    "#get first batch\n",
    "for seq in train_iter:\n",
    "    one_batch = seq\n",
    "    break\n",
    "\n",
    "total_iters = 50\n",
    "total_eopchs = 10\n",
    "tensorboard = SummaryWriter(log_dir=\".\")\n",
    "samples = 0\n",
    "for epoch in range(total_eopchs):\n",
    "    print(\"Epoch: {}/{}\".format(epoch, total_eopchs))\n",
    "    model.train(True)\n",
    "    for batch_seq in tqdm.tqdm(range(total_iters)):\n",
    "        text, label = one_batch.text[0], one_batch.label -1\n",
    "        optimizer.zero_grad()\n",
    "        if text.size(1) > max_sequence: text[:,:max_sequence]\n",
    "        pred = model(text)\n",
    "        loss = F.nll_loss(pred, label) #negstive log_likehood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        samples += text.size(0)\n",
    "        tensorboard.add_scalar('classification/train-loss', float(loss.item()), samples)\n",
    "    with torch.no_grad():\n",
    "        print(\"Train loss \", float(loss.item()))\n",
    "        model.train(False)\n",
    "        qty = 0\n",
    "        correct = 0\n",
    "        loss_val = []\n",
    "        for batch_seq in tqdm.tqdm(range(1)):\n",
    "            text, label = one_batch.text[0], one_batch.label -1\n",
    "            if text.size(1) > max_sequence: text[:, :max_sequence]\n",
    "            pred = model(text)\n",
    "            loss = F.nll_loss(pred, label)\n",
    "            \n",
    "            loss_val.append(loss.item())\n",
    "\n",
    "            pred_class = pred.argmax(dim=1)\n",
    "            qty += text.size(0)\n",
    "            correct += (pred_class==label).sum().item()\n",
    "            accuracy = float(correct) / qty\n",
    "            print(\"Acc val: {}\".format(accuracy))\n",
    "            tensorboard.add_scalar('classification/val-loss', np.array(loss_val).mean(), samples)\n",
    "            print(\"Val loss \", np.array(loss_val).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/3125 [00:08<54:36,  1.05s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-d101bc001d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#negstive log_likehood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers_from_scratch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/transformers_from_scratch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ClassSequenceTransformer(\n",
    "                                    num_classes=num_classes,\n",
    "                                    embeding_size=embeding_size,\n",
    "                                    transformer_heads=transformer_heads,\n",
    "                                    depth=depth,\n",
    "                                    vocab_size=vocab_size,\n",
    "                                    max_sequence=max_sequence,\n",
    "                                    \n",
    "                                )\n",
    "optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i:min(i/(warm/batch_size), 1.0)) # lr times lambda(iter)\n",
    "\n",
    "\n",
    "tensorboard = SummaryWriter(log_dir=\".\")\n",
    "samples = 0\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}/{}\".format(epoch, epochs))\n",
    "    model.train(True)\n",
    "    for batch_seq in tqdm.tqdm(train_iter):\n",
    "        text, label = batch_seq.text[0], batch_seq.label -1\n",
    "        optimizer.zero_grad()\n",
    "        if text.size(1) > max_sequence: text[:,:max_sequence]\n",
    "        pred = model(text)\n",
    "        loss = F.nll_loss(pred, label) #negstive log_likehood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        samples += text.size(0)\n",
    "        tensorboard.add_scalar('classification/train-loss', float(loss.item()), samples)\n",
    "    with torch.no_grad():\n",
    "        print(\"Train loss \", float(loss.item()))\n",
    "        model.train(False)\n",
    "        qty = 0\n",
    "        correct = 0\n",
    "        loss_val = []\n",
    "        for batch_seq in tqdm.tqdm(test_iter):\n",
    "            text, label = batch_seq.text[0], batch_seq.label -1\n",
    "            if text.size(1) > max_sequence: text[:, :max_sequence]\n",
    "            pred = model(text)\n",
    "            loss = F.nll_loss(pred, label)\n",
    "            \n",
    "            loss_val.append(loss.item())\n",
    "\n",
    "            pred_class = pred.argmax(dim=1)\n",
    "            qty += text.size(0)\n",
    "            correct += (pred_class==label).sum().item()\n",
    "            accuracy = float(correct) / qty\n",
    "            print(\"Acc val: {}\".format(accuracy))\n",
    "            tensorboard.add_scalar('classification/val-loss', np.array(loss_val).mean(), samples)\n",
    "            print(\"Val loss \", np.array(loss_val).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter.shuffle=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
