{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model (text generation, autoregressive model)\n",
    "\n",
    "This experiment introduce a language model based on Transformers modules. The main idea is to introduce the mask to the delf attention module. Focusing on the process of each elelement of the sequence, the mask avoids to see forward words (avoids taking into account future words of the seqence).\n",
    "The goal is to predict the next character given a seqence (input is a seqence of characters, output prediction should be the same character sequence shifted to the left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from _context import src\n",
    "from src.models.model_utils import device_selection\n",
    "from src.models.predict_model import GenerationCharacterTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.distributions as dist\n",
    "\n",
    "import numpy as np\n",
    "import random, tqdm, sys, math, gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "# NB, the enwik8 data contains tokens from 9 to 240, but well round up to the nearest\n",
    "# power of two.\n",
    "NUM_TOKENS = 256\n",
    "LOG2E = math.log2(math.e)\n",
    "\n",
    "context=256\n",
    "embeding_size = 128\n",
    "transformer_heads = 8\n",
    "depth = 4\n",
    "lr = 1e-4\n",
    "warm = 100\n",
    "iterations = 1e6\n",
    "batch_size = 8\n",
    "\n",
    "train_num = int(50e6)\n",
    "val_num = int(5e6)\n",
    "test_num = int(5e6)\n",
    "total_num = train_num + val_num + test_num\n",
    "\n",
    "test_batch = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Dataset is named enwik8 and contains $10^8$ characters of wikipedia text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enwik8\n",
    "# http://mattmahoney.net/dc/enwik8.zip\n",
    "\n",
    "path = \"./dataset/enwik8\"\n",
    "with open(path) as file:\n",
    "    arr = np.fromstring(file.read(total_num), dtype=np.uint8)\n",
    "    train_ds, val_ds, test_ds = np.split(arr, [train_num, val_num+ test_num])\n",
    "    train_ds = torch.from_numpy(train_ds)\n",
    "    val_ds = torch.from_numpy(val_ds)\n",
    "    test_ds = torch.from_numpy(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 60, 109, 101,  ...,  61,  61,  61], dtype=torch.uint8),\n",
       " torch.Size([50000000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds , train_ds.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Provide a character sequence and the model generates for each character of the seqeuence the next character. hence the last prediction of the putput sequence shuold be the next unseen letter (output is one character shiftted)\n",
    "\n",
    "<img src=\"images/transformer_model_generator.svg\"  width=\"500\" height=\"600\">\n",
    "\n",
    "It is necessary that the transformer model can not see further characters in order to perform the attention mechanism. For that reason the weighted matrix is masked with an upper diagonal of $-\\inf$ (after softmax it gets a 0)\n",
    "* Rimind that each row of th eattention weight matrix are the weight of each row sequence vector (each element of the weight vector is the weight of the row sequence vector)\n",
    "* E.g: the first row of th eweight matrix contains only the fisrt element, the rest are $-inf$. Then, it means that for the first element of the sequence the only vector that counts is the first sequence vector, the others are ignored\n",
    "<img src=\"images/mask.svg\"  width=\"500\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerationCharacterTransformer(\n",
    "                            embedding_size=128,\n",
    "                            transformer_heads=10,\n",
    "                            depth=5,\n",
    "                            max_sequence=256, #context\n",
    "                            token_size=NUM_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 256, 256]),\n",
       " tensor([[[-6.4642, -5.9990, -6.1101,  ..., -6.8322, -5.7518, -5.2660],\n",
       "          [-6.1179, -5.8189, -6.3735,  ..., -6.2957, -5.5812, -5.2820],\n",
       "          [-6.5324, -4.8010, -6.3428,  ..., -6.1939, -6.2609, -4.6662],\n",
       "          ...,\n",
       "          [-6.3499, -5.9762, -5.8542,  ..., -5.9186, -6.5017, -5.6515],\n",
       "          [-7.0291, -5.0180, -6.3818,  ..., -5.9413, -6.1559, -4.9463],\n",
       "          [-6.4592, -5.6440, -6.0255,  ..., -6.3161, -5.8701, -5.7915]],\n",
       " \n",
       "         [[-6.4642, -5.9990, -6.1101,  ..., -6.8322, -5.7518, -5.2660],\n",
       "          [-6.1179, -5.8189, -6.3735,  ..., -6.2957, -5.5812, -5.2820],\n",
       "          [-6.5324, -4.8010, -6.3428,  ..., -6.1939, -6.2609, -4.6662],\n",
       "          ...,\n",
       "          [-6.3499, -5.9762, -5.8542,  ..., -5.9186, -6.5017, -5.6515],\n",
       "          [-7.0291, -5.0180, -6.3818,  ..., -5.9413, -6.1559, -4.9463],\n",
       "          [-6.4592, -5.6440, -6.0255,  ..., -6.3161, -5.8701, -5.7915]],\n",
       " \n",
       "         [[-6.4642, -5.9990, -6.1101,  ..., -6.8322, -5.7518, -5.2660],\n",
       "          [-6.1179, -5.8189, -6.3735,  ..., -6.2957, -5.5812, -5.2820],\n",
       "          [-6.5324, -4.8010, -6.3428,  ..., -6.1939, -6.2609, -4.6662],\n",
       "          ...,\n",
       "          [-6.3499, -5.9762, -5.8542,  ..., -5.9186, -6.5017, -5.6515],\n",
       "          [-7.0291, -5.0180, -6.3818,  ..., -5.9413, -6.1559, -4.9463],\n",
       "          [-6.4592, -5.6440, -6.0255,  ..., -6.3161, -5.8701, -5.7915]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-6.4642, -5.9990, -6.1101,  ..., -6.8322, -5.7518, -5.2660],\n",
       "          [-6.1179, -5.8189, -6.3735,  ..., -6.2957, -5.5812, -5.2820],\n",
       "          [-6.5324, -4.8010, -6.3428,  ..., -6.1939, -6.2609, -4.6662],\n",
       "          ...,\n",
       "          [-6.3499, -5.9762, -5.8542,  ..., -5.9186, -6.5017, -5.6515],\n",
       "          [-7.0291, -5.0180, -6.3818,  ..., -5.9413, -6.1559, -4.9463],\n",
       "          [-6.4592, -5.6440, -6.0255,  ..., -6.3161, -5.8701, -5.7915]],\n",
       " \n",
       "         [[-6.4642, -5.9990, -6.1101,  ..., -6.8322, -5.7518, -5.2660],\n",
       "          [-6.1179, -5.8189, -6.3735,  ..., -6.2957, -5.5812, -5.2820],\n",
       "          [-6.5324, -4.8010, -6.3428,  ..., -6.1939, -6.2609, -4.6662],\n",
       "          ...,\n",
       "          [-6.3499, -5.9762, -5.8542,  ..., -5.9186, -6.5017, -5.6515],\n",
       "          [-7.0291, -5.0180, -6.3818,  ..., -5.9413, -6.1559, -4.9463],\n",
       "          [-6.4592, -5.6440, -6.0255,  ..., -6.3161, -5.8701, -5.7915]],\n",
       " \n",
       "         [[-6.4642, -5.9990, -6.1101,  ..., -6.8322, -5.7518, -5.2660],\n",
       "          [-6.1179, -5.8189, -6.3735,  ..., -6.2957, -5.5812, -5.2820],\n",
       "          [-6.5324, -4.8010, -6.3428,  ..., -6.1939, -6.2609, -4.6662],\n",
       "          ...,\n",
       "          [-6.3499, -5.9762, -5.8542,  ..., -5.9186, -6.5017, -5.6515],\n",
       "          [-7.0291, -5.0180, -6.3818,  ..., -5.9413, -6.1559, -4.9463],\n",
       "          [-6.4592, -5.6440, -6.0255,  ..., -6.3161, -5.8701, -5.7915]]],\n",
       "        grad_fn=<LogSoftmaxBackward>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fast.check\n",
    "x = torch.rand(8,256)\n",
    "x = x.long()\n",
    "o = model(x)\n",
    "o.size(), o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losss  5.660549640655518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 100/300 [03:44<07:19,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losss  0.2959350049495697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 200/300 [07:24<03:40,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losss  0.01608143374323845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [11:10<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "#One batch overfit\n",
    "tensorboard = SummaryWriter(log_dir=\".\")\n",
    "\n",
    "model = GenerationCharacterTransformer(\n",
    "                            embedding_size=256,\n",
    "                            transformer_heads=10,\n",
    "                            depth=8,\n",
    "                            max_sequence=context, #context\n",
    "                            token_size=NUM_TOKENS)\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda i:min(i/(warm/batch_size), 1.0))\n",
    "\n",
    "num_characters = train_ds.size(0)\n",
    "ixs = torch.randint(size=(batch_size,), low=0, high=num_characters - context -1)\n",
    "ixs, ixs.size()\n",
    "batch_input = [train_ds[i:i+context][None, :]for i in ix]\n",
    "batch_target = [train_ds[i+1:i+context+1][None, :]for i in ix]\n",
    "batch_input = torch.cat(batch_input, dim=0).long()\n",
    "batch_target = torch.cat(batch_target, dim=0).long()\n",
    "\n",
    "summary_loss = AverageMeter()\n",
    "for i in tqdm.tqdm(range(300)):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(batch_input)\n",
    "    #pred must be batch, classes, seq for using nll loss. So we transpose clsses by seq\n",
    "    loss = F.nll_loss(pred.transpose(2,1), batch_target, reduction=\"mean\")\n",
    "    loss_val = float(loss.detach().item())\n",
    "    if i%100 ==0:\n",
    "        print(\"Losss \", loss_val)\n",
    "    summary_loss.update(loss_val, n=batch_size)\n",
    "    tensorboard.add_scalar(\"generation/train_loss\", loss_val)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "upto = 10000\n",
    "data_sub = test_ds[:upto]\n",
    "\n",
    "with torch.no_grad():\n",
    "    bits, tot = 0.0, 0\n",
    "    batch = [] # buffer, every time it fills up, we run it through the model\n",
    "    #slide windox of size seq\n",
    "    for current in range(data_sub.size(0)):\n",
    "        f = max(0, current-context)\n",
    "        t = current + 1\n",
    "        seq = data_sub[f:t].to(torch.long)\n",
    "        #pad seq is shorter than context \n",
    "        if seq.size(0) < context+1:\n",
    "            pad = torch.zeros(size=(context+1-seq.size(0),), dtype=torch.long)\n",
    "            seq = torch.cat([pad, seq], dim=0)\n",
    "        batch.append(seq[None,:])\n",
    "        if len(batch) == test_batch or current == data_sub.size(0)-1:\n",
    "            b_size = len(batch)\n",
    "            batch = torch.cat(batch, dim=0)\n",
    "            input_batch = batch[:,:-1]\n",
    "            target_batch = batch[:,-1]\n",
    "            \n",
    "            pred = model(input_batch)\n",
    "            #For each sequence, select the probability of the target\n",
    "            lnprobs = pred[torch.arange(b_size), -1, target_batch]\n",
    "            log2probs = lnprobs * LOG2E # convert from nats to bits\n",
    "            #compute bits fot the probabilities\n",
    "            bits += - log2probs.sum()\n",
    "            batch = []\n",
    "    bits_per_byte = bits/data_sub.size(0)\n",
    "    print(\"Iteration: bit per byte\", bits_per_byte)\n",
    "    #tensorboard.add_scalar(\"generation/bits_per_byte\", bits_per_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
